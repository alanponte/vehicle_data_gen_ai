{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyO0cYdUHZF801SsT+J6pOxW"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Synthetic Vehcile Data Generator\n",
    "\n",
    "## About\n",
    "This colab notebook uses LLM models for generating synthetic vehicle test data."
   ],
   "metadata": {
    "id": "N4mBZ3XqJWwW"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Package installs"
   ],
   "metadata": {
    "id": "OWdDZH5jJgzu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Installs\n",
    "!pip install -q gradio requests torch bitsandbytes transformers accelerate openai"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E7erauNzJhaQ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1751409772588,
     "user_tz": 420,
     "elapsed": 125154,
     "user": {
      "displayName": "Alan Ponte",
      "userId": "08890713325321488831"
     }
    },
    "outputId": "34070baf-7592-46a6-869b-bf66d151b1f0"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m363.4/363.4 MB\u001B[0m \u001B[31m3.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m13.8/13.8 MB\u001B[0m \u001B[31m78.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m24.6/24.6 MB\u001B[0m \u001B[31m62.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m883.7/883.7 kB\u001B[0m \u001B[31m40.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m664.8/664.8 MB\u001B[0m \u001B[31m2.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m211.5/211.5 MB\u001B[0m \u001B[31m5.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m56.3/56.3 MB\u001B[0m \u001B[31m10.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m127.9/127.9 MB\u001B[0m \u001B[31m7.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m207.5/207.5 MB\u001B[0m \u001B[31m5.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m21.1/21.1 MB\u001B[0m \u001B[31m89.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m67.0/67.0 MB\u001B[0m \u001B[31m10.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# imports\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import io\n",
    "import json\n",
    "import gradio as gr\n",
    "import requests\n",
    "import subprocess\n",
    "import google.generativeai as ggai\n",
    "import torch\n",
    "import tempfile\n",
    "import shutil\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "from openai import OpenAI\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig"
   ],
   "metadata": {
    "id": "29dWaDDBJknn",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1751409802151,
     "user_tz": 420,
     "elapsed": 29555,
     "user": {
      "displayName": "Alan Ponte",
      "userId": "08890713325321488831"
     }
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Domain-level Exception"
   ],
   "metadata": {
    "id": "GGER2zW2J8jH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class SyntheticDataGeneratorException(Exception):\n",
    "  def __init__(self, message: str, cause: Exception):\n",
    "    self._message = message\n",
    "    self._cause = cause\n",
    "\n",
    "  @property\n",
    "  def message(self) -> str:\n",
    "    return self._message\n",
    "\n",
    "  @property\n",
    "  def cause(self) -> Exception | None:\n",
    "    return self._cause"
   ],
   "metadata": {
    "id": "uVhmM4OWJ89l",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1751409802153,
     "user_tz": 420,
     "elapsed": 8,
     "user": {
      "displayName": "Alan Ponte",
      "userId": "08890713325321488831"
     }
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## HuggingFace Setup"
   ],
   "metadata": {
    "id": "l3DuyxBwKCJK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Sign in to HuggingFace Hub\n",
    "\n",
    "hf_token = userdata.get('HF_TOKEN')\n",
    "login(hf_token, add_to_git_credential=True)\n",
    "print(\"Logged in to HuggingFace Hub\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Hdw3j9MJ_wn",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1751409805366,
     "user_tz": 420,
     "elapsed": 3216,
     "user": {
      "displayName": "Alan Ponte",
      "userId": "08890713325321488831"
     }
    },
    "outputId": "f36a868f-0423-4b13-9756-a8d2f99729d8"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Logged in to HuggingFace Hub\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## OpenAI Setup"
   ],
   "metadata": {
    "id": "wotLdGLkKKuH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "openai_client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
    "print(\"Successfully configured OpenAI client\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7EsAOzH9KIQ5",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1751409821345,
     "user_tz": 420,
     "elapsed": 409,
     "user": {
      "displayName": "Alan Ponte",
      "userId": "08890713325321488831"
     }
    },
    "outputId": "8c5d26f2-0b11-4338-d21e-ff5093e9c32e"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Successfully configured OpenAI client\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining Prompts"
   ],
   "metadata": {
    "id": "w2roSWstKSyp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "_SYSTEM_PROMPT = \"\"\"\n",
    "You are a synthetic vehicle dataset generator. Your role is to create a synthetic dataset which infers structured data schemas from business scenarios given by the user.\n",
    "The business scenarios will always relate to vehicle maintenance and telemetry data. Every vehicle belongs to a fleet. A fleet has a desired goal to keep vehicle downtime\n",
    "to a minimum.\n",
    "\n",
    "Your task is to:\n",
    "1. Understand the user's business problem(s) or use case(s).\n",
    "2. Identify key fields needed to support the scenario(s).\n",
    "3. Define appropriate field names, data types, and formats.\n",
    "4. Generate synthetic records which match the inferred schema.\n",
    "\n",
    "Guidelines:\n",
    "- Use realistic field names and values.\n",
    "- Chose sensible data types such as string, integer, double-precision, boolean,..\n",
    "- Respect logical constraints such as age-range, date-range, email-format.\n",
    "\n",
    "Before generating the data, display the inferred schema in JSON format.\n",
    "\"\"\"\n",
    "def get_system_prompt() -> str:\n",
    "  \"\"\"\n",
    "  Returns the system prompt for the synthetic data generator.\n",
    "\n",
    "  :return: The system prompt for the synthetic data generator.\n",
    "  \"\"\"\n",
    "  return _SYSTEM_PROMPT"
   ],
   "metadata": {
    "id": "TWzerMKkKQSC",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1751409914349,
     "user_tz": 420,
     "elapsed": 5,
     "user": {
      "displayName": "Alan Ponte",
      "userId": "08890713325321488831"
     }
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_user_prompt(business_problem, sample_size, file_format) -> str:\n",
    "  \"\"\"\n",
    "  Returns a prompt for generating synthetic data.\n",
    "\n",
    "  :param business_problem: The buisiness problem defined by the user.\n",
    "  :param sample_size: The max number of samples in the dataset.\n",
    "  :param file_format: The file format of the dataset.\n",
    "  :return: A prompt for generating synthetic data.\n",
    "  \"\"\"\n",
    "  user_prompt = f\"\"\"The business scenario for which I want you to generate a dataset is defined as: {business_problem}\n",
    "\n",
    "  Generate a synthetic dataset of {sample_size} rows in {file_format} format.\n",
    "  \"\"\"\n",
    "  return user_prompt"
   ],
   "metadata": {
    "id": "m9B38IWyKnFJ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1751409924782,
     "user_tz": 420,
     "elapsed": 40,
     "user": {
      "displayName": "Alan Ponte",
      "userId": "08890713325321488831"
     }
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Quantization"
   ],
   "metadata": {
    "id": "tcXig9HnKrZ-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_quantization_config(quant_type: str = \"nf4\"):\n",
    "  return BitsAndBytesConfig(\n",
    "      load_in_4bit=True,\n",
    "      bnb_4bit_use_double_quant=True,\n",
    "      bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "      bnb_4bit_quant_type=quant_type,\n",
    "  )"
   ],
   "metadata": {
    "id": "LiheJSrUKpoB",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1751409944284,
     "user_tz": 420,
     "elapsed": 3,
     "user": {
      "displayName": "Alan Ponte",
      "userId": "08890713325321488831"
     }
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## HuggingFace Model Inference"
   ],
   "metadata": {
    "id": "Wxju-e5WKwvK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def run_hfmodel_and_get_response(prompt, model_name, output_tokens):\n",
    "  \"\"\"\n",
    "  Run a HF model on a given prompt.\n",
    "\n",
    "  :param prompt: The prompt to run the model on.\n",
    "  :param model_name: The name of the model to run.\n",
    "  :param output_tokens: The number of tokens to generate.\n",
    "  :return: The generated text.\n",
    "  \"\"\"\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "  inputs = tokenizer.apply_chat_template(prompt, return_tensors=\"pt\")\n",
    "  if torch.cuda.is_available():\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "  streamer = TextStreamer(tokenizer)\n",
    "  if \"microsoft/bitnet-b1.58-2B-4T\" in model_name:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", trust_remote_code=True)\n",
    "  elif \"tiiuae/Falcon-E-3B-Instruct\" in model_name:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16 )\n",
    "  else:\n",
    "    # Use quantization\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", quantization_config=get_quantization_config())\n",
    "  outputs = model.generate(inputs, max_new_tokens=output_tokens, streamer=streamer)\n",
    "  response = tokenizer.decode(outputs[0])\n",
    "  del model, inputs, tokenizer, outputs\n",
    "  gc.collect()\n",
    "  torch.cuda.empty_cache()\n",
    "  return response"
   ],
   "metadata": {
    "id": "6NTiDvykKuZS",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1751410232315,
     "user_tz": 420,
     "elapsed": 11,
     "user": {
      "displayName": "Alan Ponte",
      "userId": "08890713325321488831"
     }
    }
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## OpenAI Model Interface"
   ],
   "metadata": {
    "id": "_C69s3wSK1-z"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ChatGPT response\n",
    "def get_chatgpt_response(prompt, model_name, output_tokens):\n",
    "  response = openai_client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=prompt,\n",
    "        max_tokens=output_tokens,\n",
    "    )\n",
    "  return response.choices[0].message.content"
   ],
   "metadata": {
    "id": "-T0BzIQuKzjo",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1751410233889,
     "user_tz": 420,
     "elapsed": 2,
     "user": {
      "displayName": "Alan Ponte",
      "userId": "08890713325321488831"
     }
    }
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gradio UI"
   ],
   "metadata": {
    "id": "AvKpELF2K5kh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "MODEL_TYPES = [\"GPT\", \"HuggingFace\"]\n",
    "OPENAI_MODEL_NAMES=[\"gpt-4o-mini\", \"gpt-4o\", \"gpt-3.5-turbo\"]\n",
    "HUGGINGFACE_MODELS=[\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    \"microsoft/bitnet-b1.58-2B-4T\",\n",
    "    \"ByteDance-Seed/Seed-Coder-8B-Instruct\",\n",
    "    \"tiiuae/Falcon-E-3B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "]\n",
    "MODEL_NAMES = {\n",
    "    \"GPT\": OPENAI_MODEL_NAMES,\n",
    "    \"HuggingFace\": HUGGINGFACE_MODELS\n",
    "}"
   ],
   "metadata": {
    "id": "i_6FaG4OK4Do",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1751410235446,
     "user_tz": 420,
     "elapsed": 1,
     "user": {
      "displayName": "Alan Ponte",
      "userId": "08890713325321488831"
     }
    }
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "with gr.Blocks() as generator_ui:\n",
    "    gr.Markdown(\"# Goodyear Business Scenario → Synthetic Dataset Generator\")\n",
    "\n",
    "    with gr.Row():\n",
    "      with gr.Column(scale=3):\n",
    "        with gr.Row():\n",
    "          dataset_size=gr.Number(value=10, label=\"Enter the number of data samples to generate.\", show_label=True)\n",
    "          format=gr.Dropdown([\"json\", \"csv\", \"txt\", \"markdown\"], label=\"Select the format for the dataset\", show_label=True)\n",
    "        with gr.Row():\n",
    "          scenario=gr.Textbox(label=\"Business Scenario\", lines=5, placeholder=\"Describe your business scenario here\")\n",
    "        with gr.Row():\n",
    "          error = gr.Markdown(visible=False)\n",
    "        with gr.Row():\n",
    "          clear = gr.Button(\"Clear Everything\")\n",
    "          submit = gr.Button(\"Generate Dataset\", variant=\"primary\")\n",
    "\n",
    "      with gr.Column(scale=1):\n",
    "          model_type = gr.Dropdown(MODEL_TYPES, label=\"Model Type\", show_label=True, info=\"Select the model type you want to use\")\n",
    "          model_name = gr.Dropdown(MODEL_NAMES[model_type.value], label=\"Model Name\", show_label=True, allow_custom_value=True, info=\"Select the model name or enter one manually\")\n",
    "          output_tokens= gr.Number(value=1000, label=\"Enter the max number of output tokens to generate.\", show_label=True, info=\"This will impact the length of the response containg the dataset\")\n",
    "\n",
    "    with gr.Row():\n",
    "      # Chatbot Interface\n",
    "        chatbot = gr.Chatbot(\n",
    "            type='messages',\n",
    "            label='Chatbot',\n",
    "            show_label=True,\n",
    "            height=300,\n",
    "            resizable=True,\n",
    "            elem_id=\"chatbot\",\n",
    "            avatar_images=(\"🧑\", \"🤖\",)\n",
    "        )\n",
    "    with gr.Row(variant=\"compact\"):\n",
    "      extract_btn = gr.Button(\"Extract and Save Dataset\", variant=\"huggingface\", visible=False)\n",
    "      file_name = gr.Textbox(label=\"Enter file name here (without file extension)\", placeholder=\"e.g. cancer_synthetic, warehouse_synthetic (no digits)\", visible=False)\n",
    "    with gr.Row():\n",
    "      markdown_preview = gr.Markdown(visible = False)\n",
    "      dataset_preview = gr.Textbox(label=\"Dataset Preview\",visible=False)\n",
    "    with gr.Row():\n",
    "      file_saved = gr.Textbox(visible=False)\n",
    "\n",
    "    def run_inference(scenario, model_type, model_name, output_tokens, dataset_size, format):\n",
    "      \"\"\"Run the model and get the response\"\"\"\n",
    "      model_type=model_type.lower()\n",
    "      print(f\"scenario: {scenario}\")\n",
    "      print(f\"model_type: {model_type}\")\n",
    "      print(f\"model_name: {model_name}\")\n",
    "      if not scenario.strip():\n",
    "        return gr.update(value=\"**Error:** Please define a scenario first!\",visible=True), []\n",
    "\n",
    "      user_prompt = get_user_prompt(scenario, dataset_size, format)\n",
    "      prompt =  [\n",
    "          {\"role\": \"system\", \"content\": get_system_prompt()},\n",
    "          {\"role\": \"user\", \"content\": user_prompt},\n",
    "      ]\n",
    "\n",
    "      if model_type == \"gpt\":\n",
    "        response = get_chatgpt_response(prompt=prompt, model_name=model_name, output_tokens=output_tokens)\n",
    "      else:\n",
    "        response = run_hfmodel_and_get_response(prompt=prompt, model_name=model_name, output_tokens=output_tokens)\n",
    "        torch.cuda.empty_cache()\n",
    "      history = [\n",
    "          {\"role\": \"user\", \"content\": scenario},\n",
    "          {\"role\": \"assistant\", \"content\": response}\n",
    "      ]\n",
    "      return gr.update(visible=False), history\n",
    "\n",
    "    def extract_dataset_string(response):\n",
    "      \"\"\"Extract dataset content between defined tags using regex.\"\"\"\n",
    "      # Remove known artificial tokens (common in HuggingFace or Claude)\n",
    "      response = re.sub(r\"<\\[.*?\\]>\", \"\", response)\n",
    "\n",
    "      # Remove system or prompt echo if repeated before dataset\n",
    "      response = re.sub(r\"(?is)^.*?<<<\", \"<<<\", response.strip(), count=1)\n",
    "\n",
    "      # 1. Match strict <<<>>>...<<<>>> tag blocks (use last match)\n",
    "      matches = re.findall(r\"<<<>>>[\\s\\r\\n]*(.*?)[\\s\\r\\n]*<<<>>>\", response, re.DOTALL)\n",
    "      if matches:\n",
    "          return matches[-1].strip()\n",
    "\n",
    "      # 2. Match loose <<< ... >>> format\n",
    "      matches = re.findall(r\"<<<[\\s\\r\\n]*(.*?)[\\s\\r\\n]*>>>\", response, re.DOTALL)\n",
    "      if matches:\n",
    "          return matches[-1].strip()\n",
    "\n",
    "      # 3. Match final fallback: take everything after last <<< as raw data\n",
    "      last_open = response.rfind(\"<<<\")\n",
    "      if last_open != -1:\n",
    "          raw = response[last_open + 3 :].strip()\n",
    "          # Optionally cut off noisy trailing notes, explanations, etc.\n",
    "          raw = re.split(r\"\\n\\s*\\n|Explanation:|Note:|---\", raw)[0]\n",
    "          return raw.strip()\n",
    "\n",
    "      return \"Could not extract dataset! Try again with a different model.\"\n",
    "\n",
    "    def extract_dataset_from_response(chatbot_history, file_name, file_type):\n",
    "      \"\"\"Extract dataset and update in gradio UI components\"\"\"\n",
    "      response = chatbot_history[-1][\"content\"]\n",
    "      if not response:\n",
    "        return gr.update(visible=True, value=\"Could not find LLM Response! Try again.\"), gr.update(visible=False)\n",
    "\n",
    "      dataset = extract_dataset_string(response)\n",
    "      if dataset == \"Could not extract dataset! Try again with a different model.\":\n",
    "        return gr.update(visible=True, value=dataset), gr.update(visible=False)\n",
    "      text = save_dataset(dataset, file_type, file_name)\n",
    "      return gr.update(visible=True, value=text), gr.update(visible=True, value=dataset)\n",
    "\n",
    "    def save_dataset(dataset, file_format, file_name):\n",
    "      \"\"\"Save dataset to a file based on the selected format.\"\"\"\n",
    "      file_name=file_name+\".\"+file_format\n",
    "      print(dataset)\n",
    "      print(file_name)\n",
    "      if file_format == \"json\":\n",
    "        try:\n",
    "          data = json.loads(dataset)\n",
    "          with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "          return \"Dataset saved successfully!\"\n",
    "        except:\n",
    "          return \"Could not save dataset! Try again in another format.\"\n",
    "      elif file_format == \"csv\":\n",
    "        try:\n",
    "          df = pd.read_csv(StringIO(dataset))\n",
    "          df.to_csv(file_name, index=False)\n",
    "          return \"Dataset saved successfully!\"\n",
    "        except:\n",
    "          return \"Could not save dataset! Try again in another format.\"\n",
    "      elif file_format == \"txt\":\n",
    "        try:\n",
    "          with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(dataset)\n",
    "          return \"Dataset saved successfully!\"\n",
    "        except:\n",
    "          return \"Could not save dataset! Try again in another format.\"\n",
    "\n",
    "    def clear_chat():\n",
    "      \"\"\"Clear the chat history.\"\"\"\n",
    "      return \"\", [], gr.update(visible=False), gr.update(visible=False)\n",
    "\n",
    "    def show_extract_btn(chatbot_history, format):\n",
    "      \"\"\"Show the extract button if the response has been displayed in the chatbot and format is not set to markdown\"\"\"\n",
    "      if chatbot_history == []:\n",
    "        return gr.update(visible=False), gr.update(visible=False), gr.update(visible=False)\n",
    "      if format == \"markdown\":\n",
    "        return gr.update(visible=True, value=chatbot_history[1][\"content\"]), gr.update(visible=False), gr.update(visible=False)\n",
    "      return gr.update(visible=False), gr.update(visible=True), gr.update(visible=True)\n",
    "\n",
    "    extract_btn.click(\n",
    "        fn=extract_dataset_from_response,\n",
    "        inputs=[chatbot, file_name, format],\n",
    "        outputs=[file_saved, dataset_preview]\n",
    "    )\n",
    "\n",
    "    chatbot.change(\n",
    "        fn=show_extract_btn,\n",
    "        inputs=[chatbot, format],\n",
    "        outputs=[markdown_preview, extract_btn, file_name]\n",
    "    )\n",
    "\n",
    "    model_type.change(\n",
    "        fn=lambda x: gr.update(choices=MODEL_NAMES[x], value=MODEL_NAMES[x][0]),\n",
    "        inputs=[model_type],\n",
    "        outputs=[model_name]\n",
    "    )\n",
    "\n",
    "    submit.click(\n",
    "        fn=run_inference,\n",
    "        inputs=[scenario, model_type, model_name, output_tokens, dataset_size, format],\n",
    "        outputs=[error, chatbot],\n",
    "        show_progress=True\n",
    "    )\n",
    "\n",
    "    clear.click(\n",
    "        clear_chat,\n",
    "        outputs=[scenario, chatbot, dataset_preview, file_saved]\n",
    "    )"
   ],
   "metadata": {
    "id": "KqvC93PGK9Nh",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1751410236195,
     "user_tz": 420,
     "elapsed": 293,
     "user": {
      "displayName": "Alan Ponte",
      "userId": "08890713325321488831"
     }
    }
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Launch UI"
   ],
   "metadata": {
    "id": "0ofsEitOLCLi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "generator_ui.launch(share=True, debug=True, inbrowser=True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 651
    },
    "id": "Qy9FgckiK_vZ",
    "outputId": "263abfc5-d9cc-443b-be64-dd445af99ca4"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
      "* Running on public URL: https://fd21213e498169c984.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div><iframe src=\"https://fd21213e498169c984.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "scenario: create a dataset for a fleet with 5 vehicles\n",
      "model_type: gpt\n",
      "model_name: gpt-4o-mini\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "ado7Zen5LPRB"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
